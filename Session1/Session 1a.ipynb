{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec8af52-1eee-4cc5-a810-c7b32a3aa06d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Intro: who are we (teaching NLP at ELTE)\n",
    "- LLM: LM, LLM - 10 minutes\n",
    "- coding LLM - from the slides\n",
    "- coding LLM flavors - completion, (prompted) infilling, chat (RAG -> embed)\n",
    "- Github Copilot functions (it can do all of the above)\n",
    "- Github Copilot install (https://github.com/features/copilot)\n",
    "- jogtisztaság -> GitHub settings / Copilot / OS + use own code\n",
    "- probabilisztikusság -> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a6a41-a0ce-4355-bc4c-0fe98a2662ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8d130-d3ae-4fa6-b1f6-87dd8688f3e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Who are we?\n",
    "\n",
    "1. Gyöngyössy, Natabara\n",
    "    - Ph.D. student at [Faculty of Informatics, ELTE](https://www.inf.elte.hu/doktori)\n",
    "    - Head of AI at XXX\n",
    "1. Nemeskey, Dávid Márk\n",
    "    - Research associate at the [Department of Digital Humanities, ELTE](https://elte-dh.hu/en/home-2/)\n",
    "    - Head of AI at the [National Laboratory for Digital Heritage](https://dh-lab.hu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a1d54-9f2b-4cbd-85b2-1894f0ee2132",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Research interests\n",
    "\n",
    "Common ground:\n",
    "- Natural Language Processing (NLP)\n",
    "- Machine learning (ML)\n",
    "- (Large) Language Models ((L)LM)\n",
    "\n",
    "Natabara\n",
    "- Spiking Neural Networks\n",
    "\n",
    "Dávid\n",
    "- Encoder models ([huBERT](https://huggingface.co/SZTAKI-HLT/hubert-base-cc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e90da-28b7-4c0d-b481-98aa3d77a6f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Natural Language Processing and Foundational Models\n",
    "\n",
    "A  (monster) course at the Faculty of Informatics, ELTE\n",
    "\n",
    "1. Traditional NLP, LM\n",
    "2. Large Language Models\n",
    "3. Multimodal Language Models\n",
    "\n",
    "Students also get hands-on experience via student projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2f746-1c70-453c-b101-a0c5e3cfcaf2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48c1f3-a640-4621-b1aa-5b61a7c8e301",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5943dd-656a-45c8-8b95-dd690674c907",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matchine Translation\n",
    "\n",
    "In the '50s and '60s, machine translation has become one of the most important research areas in CS:\n",
    "\n",
    "- the Cold War made it necessary to translate (enemy) communications;\n",
    "- computers improved quickly;\n",
    "- automatic translation seemed within arm's reach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249e815-878f-4f39-93b4-e2406d3fa113",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The theoretical model had already been invented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc3521-974a-4408-a4a5-b9fde057ce86",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The noisy channel model\n",
    "\n",
    "The **noisy channel model** (developed for telecommunications) tries to reconstruct the true signal from the noisy one received."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d517ab-0c38-42c7-9bd7-03296f790f3d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In translation (e.g. from Russian to English):\n",
    "\n",
    "1. The true signal is the English language (because _everybody_ speaks English)\n",
    "2. Russian is just some \"Babel noise\"\n",
    "3. We would like to reconstruct the true, English text from this noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20953530-60a5-49be-9048-99b2303b65a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mathematically,\n",
    "\n",
    "$$\\hat{En} = \\arg\\max_{En}P(Ru=\\hat{Ru}|En)$$\n",
    "\n",
    "Which English sentence is the most probable translation of the Russian one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abedffe-937c-4c36-b8f0-5f0b509a91b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Algorithm:\n",
    "\n",
    "1. We generate all English sentences;\n",
    "2. Translate all of them into Russian;\n",
    "3. We pick the one whose translation coincides with the original Russian sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189e4f9-824d-4a9f-9888-b8ebef90a0c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Wait!\n",
    "\n",
    "We need an English to Russian model to translate from Russian to English?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a43a0-73d3-4f75-a007-74ea275f5e87",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/pig_backwards.gif\"\n",
    "     style=\"display:block;float:none;margin-left:auto;margin-right:auto;width:60%\">\n",
    "\n",
    "Aren't we riding the horse backwards?!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c09b58-f982-4343-b824-9ffbab6527da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "$$\\hat{En} = \\arg\\max_{En}P(Ru=\\hat{Ru}|En)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ed794-2645-40ab-a242-1695f17a9c07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Apply the theorem:\n",
    "$$\\hat{En} = \\arg\\max_{En}\\frac{P(En|Ru)P(En)}{P(Ru)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ff442-2eb7-47ce-853d-21d080e591d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(Ru)$ is given:\n",
    "$$\\hat{En} = \\arg\\max_{En}P(En|Ru)P(En)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e07dc6-34ac-4358-a1be-1a3fdda4d01e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A working system\n",
    "\n",
    "$$\\hat{En} = \\arg\\max_{En}P(En|Ru)P(En)$$\n",
    "\n",
    "- $P(En|Ru)$ is the translation model\n",
    "- $P(En)$ is the **language model** that measures the \"Englishness\" of the text\n",
    "    - correctness\n",
    "    - fluidity\n",
    "    - consistency\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf056ec-9050-4d18-b637-a0152cd13cad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Generative\" LMs\n",
    "\n",
    "The definition does not specify how the probability is computed. Neither is it a requirement for the LM to be able to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17276fa7-9b54-4bc5-8f34-d3a8afb4457e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, these LMs are already useful\n",
    "- as components in recognition tasks (MT, OCR, STT)\n",
    "- for language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24afd18-cd89-42f9-b123-980743326e7d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However LMs, in the popular mind, are associated with text generation. How do we get there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979dae18-bbaa-4d92-bc98-468a37f0e08e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The chain rule\n",
    "\n",
    "Given $P(S)$, the probability of the sentence, we have\n",
    "\n",
    "$$P(S) = P(w_1w_2...w_N)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82674065-2005-4560-951e-d0a49647f3a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, using the chain rule\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(w_1w_2...w_N) &= P(w_1)P(w_2|w_1)\\cdots{}P(w_N|w_1...w_{N-1}) \\\\\n",
    "                &= \\prod_{i=1}^N P(w_i|w_1...w_{i-1})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225487b-5ca5-44bb-b24f-3e030c06d1c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "The previous context in the equation above can be very long. Different methods have been devised over the years to overcome this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e6908-3e4c-4b8b-bb7c-7250bf06b6d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### n-grams\n",
    "\n",
    "An n-gram model is a _discrete_ model that limits the context to $n-1$ words:\n",
    "\n",
    "$$ P(w_1w_2...w_N) = \\prod_{i=1}^N P(w_i|w_{i-N+1}...w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c6206-9041-44db-9658-4989a9f1d5f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bengio's neural LM\n",
    "\n",
    "The first NNLM [(Bengio, 2003)](https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html) was also an n-gram model (albeit neural):\n",
    "\n",
    "<img src=\"figures/bengio.webp\"\n",
    "     style=\"display:block;float:none;margin-left:auto;margin-right:auto;width:60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c13f7-6c93-4544-809b-014a77e9e2df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks compress the context into a fixed sized state, which is updated after each step:\n",
    "\n",
    "<a href=\"https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/\"><img src=\"figures/rnn.png\"\n",
    "     style=\"display:block;float:none;margin-left:auto;margin-right:auto;width:80%\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafdfaac-66de-48be-9fdb-05cfa63a6017",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "During training, the model can be unrolled and trained parallelly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381880c-93a2-4bea-b9c4-634b425e70a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Long-Short Term Memory (LSTM)\n",
    "\n",
    "RNNs suffer from the vanishing / exploding gradient issue, and cannot model long-term dependencies. LSTMs [(Hochreiter, 1997)](https://ieeexplore.ieee.org/abstract/document/6795963) alleviate this issue by used a gated architecture:\n",
    "\n",
    "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\"><img src=\"figures/LSTM3-chain.png\"\n",
    "     style=\"display:block;float:none;margin-left:auto;margin-right:auto;width:60%\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948fcb2-f626-438b-b6a6-7f51392582cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The LSTM became the first popular neural LM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdaa9f-e842-41a4-bf96-40634dd9ff41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformer\n",
    "\n",
    "The Transformer [(Vaswani, 2017)](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) is a MT architecture. It has fixed, but large context window(s), and all previous timestamps are available via the **attention** mechanism:\n",
    "\n",
    "<a href=\"https://www.researchgate.net/figure/Transformer-model-as-proposed-by-Vaswani-et-al-3_fig1_328627493\">\n",
    "  <img src=\"figures/transformer.png\"\n",
    "     style=\"display:block;float:none;margin-left:auto;margin-right:auto;width:20%\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd295266-dcee-44fa-a9ad-7e509e59b952",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformer $-$ cont.\n",
    "\n",
    "The full transformer is an encoder-decoder architecture. Different parts are used for different purposes:\n",
    "\n",
    "1. The full Transformer for MT, sometimes LM;\n",
    "2. The encoder as _contextual embedding_;\n",
    "3. The decoder as autoregressive LM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba5774-c54c-4a83-838d-8e4d692f1a28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Large Language Models (LLMs)\n",
    "\n",
    "Most \"famous\" language models use the Transformer decoder architecture. Their size has grown exponentially in the last 5 years:\n",
    "\n",
    "<a href=\"https://medium.com/@harishdatalab/unveiling-the-power-of-large-language-models-llms-e235c4eba8a9\">\n",
    "  <img src=\"figures/model_size_growth2.png\"\n",
    "     style=\"display:block;float:none;margin-left:auto;margin-right:auto;width:60%\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb0496-9609-4501-89d6-674da4e96efa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Emergent properties\n",
    "\n",
    "As the size of the LM grows, two things happen:\n",
    "\n",
    "1. It needs more and more training data as well (for pretraining)\n",
    "2. Emergent capabilities (reasoning, prompting, etc.) appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8c2fb-1e36-4f67-91f2-48abed48ab32",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are of course many downsides as well:\n",
    "\n",
    "1. Barriers to entry\n",
    "2. Environmental effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a38e8d-ea7f-470a-bc00-345cd7b23150",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LLM training\n",
    "\n",
    "LLM training has three main stages:\n",
    "\n",
    "1. _Pretraining_: the model \"reads\" a **lot** of text (next word prediction task) $-$ nowadays in the order of 2 trillion tokens for English;\n",
    "2. _Instruction fine-tuning_: trains the model for instruction following with example instructions;\n",
    "3. _Alignment_: tries to remove bias from the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38c3bd-633b-4dba-ac16-c679e52a02fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Code LLMs\n",
    "\n",
    "Many LLMs use some source code as part of their\n",
    "pretraining corpus:\n",
    "- it helps regular models in reasoning and some level of code generation\n",
    "- **coding models** are trained explicitly for the latter task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc99ab-6627-4b88-804f-7f5f9da53f67",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coding LLM functions\n",
    "\n",
    "Coding LLMs support three main functions:\n",
    "\n",
    "1. _Completion_ of code and comments (→)\n",
    "2. _Infilling_: predicting the missing part of a program given the surrounding context (⇄)\n",
    "3. _Instruction following_ to allow assistant functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987ded7-0d3d-48cd-a087-1a8d1bedc102",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## An example: Code Llama\n",
    "\n",
    "[Code Llama](https://arxiv.org/abs/2308.12950) is a model based on LLaMa 2 and is available in the same sizes.\n",
    "\n",
    "It has three versions:\n",
    "\n",
    "- _Code Llama_: the basic model\n",
    "- _Code Llama_ - Instruct: fine-tuned\n",
    "- _Code Llama_ - Python: further trained on Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad4136-4e7b-470f-a737-15d5f001c95c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code Llama Details\n",
    "\n",
    "Training corpus (500B):\n",
    "\n",
    "- 85\\% source code from GitHub;\n",
    "- 8\\% code-related NL discussions (StackOverflow, etc.);\n",
    "- 7\\% natural language batches to retain NLU performance.\n",
    "\n",
    "The Python model is trained with 100B additional tokens of Python code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13359af4-34f2-43be-b9fe-0dfcef9f97d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code Llama Details - cont.\n",
    "\n",
    "![Code Llama pipeline. Stages are annotated with the number of training tokens.](figures/code_llama.png)\n",
    "\n",
    "- Only the smaller models support _infilling_ / _completion_\n",
    "- Long input context (4k -> 100k) enables _repository-level reasoning_\n",
    "- Llama 2 base + instruct fine-tuning facilitates _assistant functionality_  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444aa1c7-7cd9-45f0-9a05-09847ee41594",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other models\n",
    "\n",
    "Closed:\n",
    "\n",
    "  - [AlphaCode](https://arxiv.org/abs/2203.07814)\n",
    "  - [phi-1](https://huggingface.co/microsoft/phi-1)\n",
    "  - [GPT-4](https://arxiv.org/abs/2303.08774)\n",
    "\n",
    "Open:\n",
    "\n",
    "  - [SantaCoder](https://huggingface.co/bigcode/santacoder)\n",
    "  - [StarCoder](https://huggingface.co/blog/starcoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67cafbf-98c8-4609-96fa-b48ee5660f70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# GitHub Copilot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8083116-3a20-434a-b131-6796fbf66587",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Installing Copilot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f65a3a-47cd-4c8f-89b2-2b0a31204e4e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Register for Copilot [here](https://github.com/features/copilot/plans):\n",
    "- individual / business plans are available\n",
    "- students and teachers may request free access [here](https://github.com/edu/teachers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdea79d-8ae2-461b-bb31-6c9fa53290f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Install the \"GitHub Copilot\" extension in VS Code\n",
    "  <span style=\"display:inline-block;margin-left:50px\">\n",
    "  <img style=\"vertical-align:middle\" src=\"figures/extensions_button.png\" alt=\"Extensions button\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13eb2d5-25c6-4c75-a765-52d5ae9359ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Log in to GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123cc77-3434-4b14-8f9f-216e1a1e168b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A short demo\n",
    "\n",
    "short demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9d589-f809-4da5-aff3-61e7b8686fc2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Good to Know\n",
    "\n",
    "- licensing issues and copyright\n",
    "- it's a statistical model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086fc878-0120-4bea-bf48-1ae9181928d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Licensing issues\n",
    "\n",
    "- Copilot may suggest open source code [_in violation of copyright laws and software licensing requirements_](https://www.theregister.com/2024/01/12/github_copilot_copyright_case_narrowed/)\n",
    "- GitHub (and others) may use your code snippets _right from your IDE_ as training data\n",
    "\n",
    "You can opt out from both of these \"features\" during registration or on the [_Your Copilot_](https://github.com/settings/copilot) setting page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099fb7a-f4a8-4a7e-9628-f1b4b9c41916",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Opt out during registration\n",
    "\n",
    "![Opt out during registration](figures/copilot_privacy.png \"Opt out during registration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610ba21-286a-4345-875c-59f5479660aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stochasticity\n",
    "\n",
    "Copilot is an LLM, which is inherently stochastic due to the\n",
    "\n",
    "- random seed used\n",
    "- timing differences in the GPU / service\n",
    "- model updates behind the service\n",
    "- etc.\n",
    "\n",
    "Because of this, we _cannot expect to get the same completion_ for the same input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "rise": {
   "theme": "solarized"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
